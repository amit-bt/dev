{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim import models, corpora\n",
    "from gensim import similarities as sim\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_file = 'D:\\\\\\\\DS\\\\\\\\L2_Support_POC\\\\\\\\JCI_data\\\\\\\\Ticket_Data_JCI BW.xlsx'\n",
    "JCI_pdf = pd.read_excel('D:\\\\\\\\DS\\\\\\\\L2_Support_POC\\\\\\\\JCI_data\\\\\\\\Ticket_Data_JCI BW.xlsx')\n",
    "#JCI_pdf = pd.read_excel(data_file)\n",
    "JCI_pdf = JCI_pdf.dropna(subset=['Resolution'])\n",
    "\n",
    "data = JCI_pdf.Resolution.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 11\n",
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "for text in data:\n",
    "    #print(text)\n",
    "    tokenized_data.append(clean_text(text))\n",
    "\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    "#print(dictionary.keys())\n",
    "\n",
    "for key, value in dictionary.items():\n",
    "    print(key, \" : \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))\n",
    "\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSI model\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "print(\"LSI Model:\")\n",
    "\n",
    "topic_list = []\n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    #print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 10)[6:])\n",
    "    all_topics = lsi_model.print_topic(idx, 10)\n",
    "    topic_list_temp = [idx]\n",
    "    for i in all_topics.split('+'):\n",
    "        topic_list_temp.append(i[8:])\n",
    "    topic_list.append(topic_list_temp)\n",
    "print(len(topic_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_max_val(model_bow):\n",
    "    max_idx = 0\n",
    "    max_val = 0\n",
    "    for tup in model_bow:\n",
    "        temp_idx = tup[0]\n",
    "        temp_val = tup[1]\n",
    "        if temp_val > max_val:\n",
    "            max_idx = temp_idx\n",
    "            max_val = temp_val\n",
    "    return (max_idx, max_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JCI_lda_classify_pdf = JCI_pdf\n",
    "JCI_lda_classify_pdf['topic#'] = 0\n",
    "JCI_lda_classify_pdf['terms'] = ''\n",
    "\n",
    "JCI_lsi_classify_pdf = JCI_pdf\n",
    "JCI_lsi_classify_pdf['topic#'] = 0\n",
    "JCI_lda_classify_pdf['terms'] = ''\n",
    "\n",
    "for row in JCI_lda_classify_pdf.itertuples():\n",
    "    bow = dictionary.doc2bow(clean_text(str(row.Resolution)))\n",
    "    model_bow = lda_model[bow]\n",
    "    max_idx, max_val = find_max_val(model_bow)\n",
    "    JCI_lda_classify_pdf.loc[row.Index, 'topic#'] = max_idx\n",
    "    #print(topic_list[0][0])\n",
    "    for idx, val in enumerate(topic_list):\n",
    "        if(topic_list[idx][0] == max_idx):\n",
    "            JCI_lda_classify_pdf.loc[row.Index, 'terms'] = str(topic_list[max_idx])\n",
    "            break\n",
    "            \n",
    "    \n",
    "for row in JCI_lsi_classify_pdf.itertuples():\n",
    "    bow = dictionary.doc2bow(clean_text(str(row.Resolution)))\n",
    "    model_bow = lsi_model[bow]\n",
    "    max_idx, max_val = find_max_val(model_bow)\n",
    "    JCI_lsi_classify_pdf.loc[row.Index, 'topic#'] = max_idx\n",
    "    for idx, val in enumerate(topic_list):\n",
    "        if(topic_list[idx][0] == max_idx):\n",
    "            JCI_lsi_classify_pdf.loc[row.Index, 'terms'] = str(topic_list[max_idx])\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
